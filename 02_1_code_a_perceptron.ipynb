{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/silvalugo/deep_learning/blob/main/02_1_code_a_perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHoT4kJRb4zZ"
      },
      "source": [
        "![Practicum AI Logo image](images/practicum_ai_logo.png) <img src='images/practicumai_deep_learning.png' alt='Practicum AI: Deep Learning Foundations icon' align='right' width=50>\n",
        "\n",
        "\n",
        "***\n",
        "# *Practicum AI:* Deep Learning - Perceptron\n",
        "\n",
        "\n",
        "> This exercise adapted from the [W3 Schools Perceptrons](https://www.w3schools.com/ai/ai_perceptrons.asp) article and from Baig et al. (2020) The Deep Learning Workshop from [Packt Publishers](https://www.packtpub.com/product/the-deep-learning-workshop/9781839219856) (Exercise 2.01, page 55).\n",
        "\n",
        "<img alt=\"A cartoon of Dr. Amelia, a nutrition researcher, sitting at a computer thinking about food items which appear in a thought bubble.\" src=\"images/DrAmelia.jpg\" align=\"right\" width=250>Amelia is back! This time, she needs your help to analyze some of her survey data. As part of Amelia's dietary study, participants are also asked to follow a special nutrition plan, the Dr. Amelia Recommended Nutrition Plan (the DARN Plan). We'll use a simple [perceptron](https://developers.google.com/machine-learning/glossary#perceptron) to predict if participants follow the DARN Plan.\n",
        "\n",
        "**Note:** Dr. Amelia's cartoon was generated with AI's assistance.\n",
        "\n",
        "As a note, this exercise lies somewhere between coding everything from scratch and relying on the pre-coded APIs (Application Programming Interfaces) that underlie the power of TensorFlow, Keras, and Pytorch. **You will not need to create weight tensors beyond this exercise**. Still, hopefully, by doing it this time, you will have a better understanding (*and appreciation*) of the details often lost in an API call to `model.fit()`, for example.\n",
        "\n",
        "The table below shows some data Amelia has gathered from participant surveys about their nutrition. She is looking at how different factors predict if participants follow her DARN Plan ($y$, the output or [labels](https://developers.google.com/machine-learning/glossary#label) in our example) based on three input variables: if participants submit photos of three meals a day ($x_1$), if participants report being satisfied with their food choices ($x_2$), and if participants report being generally happy ($x_3$). We will combine $x_1$, $x_2$, and $x_3$ into our input tensor $X$. Here, we are simplifying the question of the likelihood of following the DARN Plan to a Yes/No.\n",
        "\n",
        "Case # | Photos of 3 meals submitted? ($x_1$) | Satisfied with food choices? ($x_2$) | Generally happy? ($x_3$) | Following the DARN Plan? ($y$)\n",
        "--|--------------------------|---------------------|-----------------------|----------------\n",
        "1 | 1 (Yes) | 1 (Yes) | 1 (Yes) | Yes (1)\n",
        "2 | 0 (No) | 1 (Yes) | 1 (Yes) | Yes (1)\n",
        "3 | 1 (Yes) | 0 (No) | 1 (Yes) | Yes (1)\n",
        "4 | 0 (No) | 0 (No) | 1 (Yes) | Yes (1)\n",
        "5 | 1 (Yes) | 1 (Yes) | 0 (No) | Yes (1)\n",
        "6 | 0 (No) | 1 (Yes) | 0 (No) | No (0)\n",
        "7 | 1 (Yes) | 0 (No) | 0 (No) | No (0)\n",
        "8 | 0 (No) | 0 (No) | 0 (No) | No (0)\n",
        "\n",
        "\n",
        "## 1. Import libraries\n",
        "\n",
        "### <img src='images/note_icon.svg' width=40, align='center' alt='Note icon'> Note\n",
        "\n",
        "> * We'll probably stop reminding you after this, but... remember not all red output is bad!\n",
        "> * Also, remember to check that the correct kernel is selected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yprAlWWpb4zc"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from tensorflow.keras import activations\n",
        "\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rh9-OM3rb4zc"
      },
      "source": [
        "## 2. Create an input data matrix\n",
        "\n",
        "Create a 3 x 8 matrix for our input data. Remember that we have three input variables (we'll call them $x_1$, $x_2$, and $x_3$ for now). These variables are the columns in our input data.\n",
        "\n",
        "The matrix below has the three input columns of our data table, using just the 0/1 values corresponding to the no/yes entries in the table. The comments help match rows of the table with entries in our `X` variable. (Remember, we are using the capital letter `X` as our variable name here to remind us that this is a matrix with our input data)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24x3nmgeb4zc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9486e5bb-a7df-4354-c59d-3ca901ba47df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tf.Variable 'Variable:0' shape=(8, 3) dtype=float32, numpy=\n",
            "array([[1., 1., 1.],\n",
            "       [0., 1., 1.],\n",
            "       [1., 0., 1.],\n",
            "       [0., 0., 1.],\n",
            "       [1., 1., 0.],\n",
            "       [0., 1., 0.],\n",
            "       [1., 0., 0.],\n",
            "       [0., 0., 0.]], dtype=float32)>\n"
          ]
        }
      ],
      "source": [
        "X = tf.Variable([[1.,1.,1.], # Case 1\n",
        "                 [0.,1.,1.], # Case 2\n",
        "                 [1.,0.,1.], # Case 3\n",
        "                 [0.,0.,1.], # Case 4\n",
        "                 [1.,1.,0.], # Case 5\n",
        "                 [0.,1.,0.], # Case 6\n",
        "                 [1.,0.,0.], # Case 7\n",
        "                 [0.,0.,0.]], # Case 8\n",
        "                 dtype = tf.float32)  # 3x8, input data table\n",
        "print(X)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4MDV6QMb4zd"
      },
      "source": [
        "## 3. Create a label tensor\n",
        "\n",
        "* Create a tensor of labels to hold our 'ground truth'. This indicates, for each set of input, whether or not the participant is following the DARN Plan.\n",
        "\n",
        "* The y variable is initially a 1D tensor (vector) with 8 elements. Reshaping it to [8, 1] converts it from a 1D vector to a 2D tensor (matrix) with 8 rows and 1 column.\n",
        "\n",
        "* Reshaping is necessary because y will be used in a matrix operation, and matrices dimensions need to be aligned. The output y is represented as a column vector with the shape [num_samples, 1] or 2D instead of 1D array. By reshaping to [8, 1], we are conforming to the expected format for further operations like loss calculation, which often involves matrices or vectors with two dimensions.\n",
        "\n",
        "```python\n",
        "# Outputs:       1, 2, 3, 4, 5, 6, 7, 8 - one for each case in the table         \n",
        "y = tf.Variable([1, 1, 1, 1, 1, 0, 0, 0], dtype = tf.float32)\n",
        "\n",
        "# Reshape to be 8 rows of 1 column  \n",
        "y = tf.reshape(y, [8,1])\n",
        "print(y)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Slj-AV2vb4zd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a485bd8-0505-41f6-8f24-59a72b4304bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]], shape=(8, 1), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "y = tf.Variable([1, 1, 1, 1, 1, 0, 0, 0], dtype = tf.float32)\n",
        "y = tf.reshape(y, [8,1])\n",
        "print(y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKtOn6twb4zd"
      },
      "source": [
        "## 4. Define some constants to set the shape of the weight matrix\n",
        "\n",
        "Define two constants to be used in the next step when we define the connections weight matrix.\n",
        "\n",
        "We can use the number of columns in the X table to determine the number of features or how many $x_i$ we have and, therefore, how many weights we need to store (one for each feature). We only need one output value since we are looking for a binary decision about plan adherence (Yes/No)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Shape is a function that returns the dimension of the data frame\n",
        "*   num_features = X.shape[1]: This gets the number of features (or columns) in the input data matrix X.\n",
        "*   If X is a matrix with dimensions (m, n), where m is the number of rows (samples) and n is the number of columns (features), then:\n",
        "\n",
        "    *   X.shape[0] gives the number of rows (m).\n",
        "    *   X.shape[1] gives the number of columns (n), which is the total number of features.\n",
        "*   output_size = 1: This indicates that the model is expected to produce a single output.\n",
        "*   In a machine learning context, if output_size = 1, it often means the model is performing a regression task or binary classification, where only one output value is needed per input. For example, in a regression task, the model predicts a single continuous value, and in binary classification, it predicts a probability or a binary class label, which it is our case with neural network.\n",
        "\n"
      ],
      "metadata": {
        "id": "R890TTZFpiPa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lydk-p2Bb4zd"
      },
      "source": [
        "```python\n",
        "num_features = X.shape[1]\n",
        "output_size = 1\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkQS4ysqb4ze"
      },
      "outputs": [],
      "source": [
        "num_features = X.shape[1]\n",
        "output_size = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Z097eB9b4ze"
      },
      "source": [
        "***\n",
        "\n",
        "## 5. Define connections weight matrix\n",
        "\n",
        "![Diagram of the perceptron with 3 input variables (x1, x2, x3), 3 weights (w1, W2, w3) and the bias term. The perceptron body multiplies the inputs by the weights and sums them and the bias, resulting in the output--whether or not the participant is following the DARN Plan. The three weights are highlighted here.](images/02_perceptron_section5.png)\n",
        "\n",
        "In our feature matrix, we will need one weight for each feature, $x_i$ (three photos submitted, satisfied with food choices, etc.), labeled $X$. These weights are our $w_i$. We don't know what value they should take so we will initialize them with a random, positive number - this is one reason different runs of model training may give different answers. Another common option is to use 0 to initialize the weights, though that can have issues in training.\n",
        "\n",
        "* tf.Variable: Wraps the tensor in a tf.Variable, making it a trainable variable. This means it can be updated during training (e.g., weights in a neural network).\n",
        "* tf.random.uniform generates values spread evenly across a range (e.g., between -1 and 1), which could lead to larger values being equally likely to occur as small ones.\n",
        "* This may not be ideal in some cases, as starting with larger values can slow down the learning process or lead to exploding gradients.\n",
        "* tf.random.uniform([num_features, output_size]):\n",
        "  * This generates a tensor of random values with a uniform distribution. The shape of the tensor is [num_features, output_size].\n",
        "  * num_features: The number of rows in the tensor, which corresponds to the number of input features (independent variables). We must use num_features because we need a weigh for each variable.\n",
        "  * output_size: The number of columns in the tensor, which corresponds to the size of the model’s output (number of predictions or output neurons).\n",
        "  * Since we are performing a matrix multiplication later, the dimensions need to match the number of input features and outputs.\n",
        "\n",
        "* minvalue=0:\n",
        "  * Specifies that the minimum value for the uniform distribution is 0. The generated values will lie between 0 and 1 (since no maxvalue is specified, 1 is the default upper bound for tf.random.uniform).\n",
        "\n",
        "* dtype=tf.float32:\n",
        "  *Ensures that the tensor's elements are stored as 32-bit floating-point numbers.\n",
        "\n",
        "\n",
        "```python\n",
        "W = tf.Variable(tf.random.uniform([num_features, output_size]), minvalue=0, dtype = tf.float32)\n",
        "print(W)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKFAjydRb4ze",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fc92d1b-030b-40ae-fcfd-fe870f0c2c7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tf.Variable 'Variable:0' shape=(3, 1) dtype=float32, numpy=\n",
            "array([[0.8897692 ],\n",
            "       [0.24160457],\n",
            "       [0.22348213]], dtype=float32)>\n"
          ]
        }
      ],
      "source": [
        "W =tf.Variable(tf.random.uniform([num_features, output_size]), minvalue=0, dtype = tf.float32)\n",
        "print(W)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMEz8fd9b4zf"
      },
      "source": [
        "***\n",
        "\n",
        "## 6. Define bias variable\n",
        "\n",
        "![Diagram of the perceptron with 3 input variables (x1, x2, x3), 3 weights (w1, W2, w3) and the bias term. This is similar to the above image, but is highlighting the bias term](images/02_perceptron_section6.png)\n",
        "\n",
        "Since we only have one neuron, we only need one bias value. Again, we'll initialize it to a random number - 0 would be another option here. We can write each bias term as $b_i$ and the matrix of all biases as $B$.\n",
        "\n",
        "* tf.random.normal generates values centered around a mean (often 0) with a certain standard deviation, leading to more values clustered near the mean and fewer extreme values.\n",
        "* This is often suitable for initializing weights and biases in deep learning models, where values close to 0 can help the model start with small, stable updates during training.\n",
        "* In this case, the biases B are initialized with a normal distribution because many models perform better when weights and biases are initialized with small values near 0.\n",
        "* tf.random.normal([output_size, 1]):\n",
        "  * This generates a tensor with random values drawn from a normal distribution.\n",
        "  * The shape of the tensor is [output_size, 1], meaning it has output_size rows and 1 column. So, the number of rows equal to the output_size =1.\n",
        "  * We must use output_size because we only need one bias in this example.\n",
        "* dtype=tf.float32: Ensures that the elements of the tensor are 32-bit floating-point numbers.\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "B = tf.Variable(tf.random.normal([output_size, 1]), dtype = tf.float32)\n",
        "print(B)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTfPc8qEb4zf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49f0270c-3d47-4c7b-e7a9-9d927b2f47b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tf.Variable 'Variable:0' shape=(1, 1) dtype=float32, numpy=array([[0.10403337]], dtype=float32)>\n"
          ]
        }
      ],
      "source": [
        "B = tf.Variable(tf.random.normal([output_size, 1]), dtype = tf.float32)\n",
        "print(B)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRiFAYiyb4zf"
      },
      "source": [
        "***\n",
        "\n",
        "## 7. Define a perceptron function\n",
        "\n",
        "![Diagram of the perceptron with 3 input variables (x1, x2, x3), 3 weights (w1, W2, w3) and the bias term. This is similar to the above image, but is highlighting the perceptron body.](images/02_perceptron_section7.png)\n",
        "\n",
        "\n",
        "In the following code block, we define a perceptron function with one input argument, $X$, containing our three input data features.\n",
        "\n",
        "The function's first line implements a net input function.  It multiplies the input data matrix ($X$) by the weights ($W$) using the matrix multiplication function (matmul).  It then adds the bias ($B$) value to that product.\n",
        "\n",
        "### <img src='images/note_icon.svg' width=40, align='center' alt='Note icon'>Note\n",
        "> This is the essential function of a neuron: gather the inputs, multiply each input by the weight for that input, add the products up and add in the bias.\n",
        "\n",
        "The function's second line implements an activation function. The activation function determines how the neuron's output (calculated above) is changed before passing it on. Here, we use the `tanh` activation function.  However, there are other TensorFlow options.  For example, you could use the `tf.sigmoid` function.  Or, select a function from the Keras activation (`activations`) library.  Search the [Keras documentation](https://keras.io/api/layers/activations/) for a complete list of available functions.\n",
        "\n",
        "Try out these other options, retrain the network, and see what happens.\n",
        "\n",
        "```python\n",
        "output = tf.sigmoid(z)\n",
        "output = activations.relu(z)\n",
        "output = activations.linear(z)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raldGnlcb4zf"
      },
      "outputs": [],
      "source": [
        "def perceptron(X):\n",
        "    z = tf.add(tf.matmul(X, W), B)  # Net input function\n",
        "    output = tf.tanh(z)             # Activation function\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcT1_gl_b4zf"
      },
      "source": [
        "Execute the perceptron function to see its initial predictions before any training.  All of its predictions ought to be 0 (remember we set all the weights and the bias to 0 - so whatever the inputs are, they are all multiplied by 0 and have 0 added to the sum)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYPTt6U5b4zg",
        "outputId": "ae67cefe-4645-454f-998a-3ea7a6e8713a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[0.8974366 ]\n",
            " [0.51471275]\n",
            " [0.8388514 ]\n",
            " [0.31628656]\n",
            " [0.84414065]\n",
            " [0.33250135]\n",
            " [0.7589791 ]\n",
            " [0.10365966]], shape=(8, 1), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "# Execute the perceptron to see its initial predictions before training.\n",
        "print(perceptron(X))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GVU2fYYb4zg"
      },
      "source": [
        "## 8. Training the Perceptron\n",
        "\n",
        "* Now that we have the elements of a simple, single-node perceptron in place, let's train the network using an algorithm called \"stochastic gradient descent\" (SGD). The purpose of SGD is to iteratively adjust the weights and bias parameters of the single neuron in our model and eventually, we hope, find values that make our neuron's predictions as good as possible. Tensorflow/Keras implements this algorithm for us, so we don't need to code it ourselves.\n",
        "\n",
        "* The [learning rate](https://developers.google.com/machine-learning/glossary#learning-rate) determines the size of the steps taken towards the global minimum. Here, the Stochastic Gradient Descent (SGD) optimizer has been selected.\n",
        "\n",
        "* The learning rate is a floating-point number that tells the gradient descent algorithm how strongly to adjust weights and biases on each iteration. For example, a learning rate of 0.3 would adjust weights and biases three times more powerfully than a learning rate of 0.1.\n",
        "\n",
        "* Learning rate is a key hyperparameter. If you set the learning rate too low, training will take too long. If you set the learning rate too high, gradient descent often has trouble reaching convergence.\n",
        "\n",
        "* optimizer = tf.optimizer.SGD(learning_rate)\n",
        "  * tf: This is the TensorFlow library, which provides tools for building and training machine learning models.\n",
        "\n",
        "  * optimizers: This is a module within TensorFlow that contains various optimization algorithms used for training machine learning models. It helps in adjusting the model parameters (like weights) to minimize the loss function.\n",
        "\n",
        "  * SGD: This is a class (or function-like object) within the tf.optimizers module that implements the Stochastic Gradient Descent (SGD) optimization algorithm. It's used for updating model parameters based on the gradients computed during backpropagation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZALvS2Fab4zg"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.01\n",
        "optimizer = tf.optimizers.SGD(learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Mkkwjf9b4zg"
      },
      "source": [
        "## 9. Train the perceptron for 1000 epochs\n",
        "\n",
        "An [epoch](https://developers.google.com/machine-learning/glossary#epoch) is a complete training pass over the entire dataset. Our loss or error function is defined as a lambda function (a single-line, inline function) in the first line of code in the loop block.  We use the `sigmoid_cross_entropy_with_logits` function, an appropriate choice for this application, to calculate how far our predicted results are from the known results. We will not get into the technical details here as that is outside the scope of this learning experience. Our SGD optimizer seeks to minimize the model's total error in the second line.\n",
        "\n",
        "### <img src='images/note_icon.svg' width=40, align='center' alt='Note icon'>Note\n",
        "> The code below uses a `for` loop. This common programming construct allows you to loop, or iterate, through\n",
        "> a list of items (the numbers 0 to 999 in our case). *Implicitly*, training will use `for` loops - for each epoch do\n",
        "> this thing. *Explicitly*, however, after this notebook, we will use the API that automatically does this for us.\n",
        "> Thus we dropped coverage of `for` loops and other \"flow control\" methods from the *Python for AI* course. It's\n",
        "> helpful to know about them, but they are rarely used explicitly in AI research.\n",
        "> [Click here for more details](https://wiki.python.org/moin/ForLoop).\n",
        ">\n",
        "> The block also uses a special Python function called a `lambda` function. These are functions that can be\n",
        "> written as a single line of code. [Click here for more\n",
        "> details](https://docs.python.org/3/tutorial/controlflow.html#lambda-expressions).\n",
        "\n",
        "Explanation of the script:\n",
        "* Loop: The model is trained over multiple epochs:\n",
        "  * for n in range(no_of_epochs:\n",
        "  * This is a loop that runs for a specified number of epochs (no_of_epochs).\n",
        "  * An epoch is a full pass through the training dataset.\n",
        "  * This is the total number of epoch the loss will be caculated.\n",
        "  * Training typically involves multiple epochs to allow the model to learn better by iteratively updating its parameters.\n",
        "* Loss Calculation: For each epoch, a lambda function calculates the loss using sigmoid cross-entropy between the true labels and the model's predictions.\n",
        "  * loss: It calculates the loss value for the current state of the model.\n",
        "  * lambda: The function calculates the loss when called, and is necessary here because optimizer.minimize expects a function as its first argument.\n",
        "  * abs(...): The abs() function takes the absolute value of the computed loss. In typical scenarios, this might be unnecessary because the cross-entropy loss is non-negative, but it ensures that the loss value is always non-negative.\n",
        "  * tf.reduce_mean(...): This calculates the mean of the cross-entropy losses across all the samples in the batch, giving you a single scalar value representing the average loss.\n",
        "  * tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=perceptron(X)): This computes the neural network sigmoid cross-entropy loss between the true labels y and the model's logits. This loss function is commonly used for binary classification problems.\n",
        "  * perceptron(X): This is the forward pass of a neural network (or a perceptron model) applied to the input data X, producing the logits (raw, unscaled predictions) before applying any activation function.\n",
        "\n",
        "* Optimization: The optimizer updates the model parameters (W and B) to minimize the calculated loss by the lambda function, improving the model's performance over time.\n",
        "  * optimizer: This is an instance of an optimizer, such as Stochastic Gradient Descent (SGD), which adjusts the model's weights and biases to minimize the loss.\n",
        "  * minimize(loss, [W, B]): This method computes the gradients of the loss with respect to the variables [W, B] (which are the model's weights and biases) and then updates these variables in the direction that reduces the loss.\n",
        "  * loss: The function passed to minimize, which calculates the loss value for the current state of the model.\n",
        "\n",
        "This loop continues until the specified number of epochs is reached, iteratively improving the model by minimizing the loss function at each step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "YEgBi2fZb4zg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "1bc1e5e8-697d-470b-a7f9-8f10648f43e0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'SGD' object has no attribute 'minimize'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-29f905b12f88>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_of_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperceptron\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'SGD' object has no attribute 'minimize'"
          ]
        }
      ],
      "source": [
        "no_of_epochs = 1000\n",
        "\n",
        "for n in range(no_of_epochs):\n",
        "    loss = lambda:abs(tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels = y, logits = perceptron(X))))\n",
        "    optimizer.minimize(loss, [W, B])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The error message 'SGD' object has no attribute 'minimize' indicates that the SGD optimizer you're using does not have a method named minimize. In TensorFlow, the method for minimizing a loss function has been updated in recent versions. Here’s how to fix the issue.\n",
        "\n",
        "Probably I am using TensorFlow 2.x. In this case, you typically use tf.GradientTape to compute gradients and then apply updates. The script is presented below, and here is the explanation:\n",
        "\n",
        "Explanation of the Modified Code:\n",
        "\n",
        "* tf.GradientTape():\n",
        "  * This context manager records the operations for automatic differentiation. When you calculate the loss, it tracks the gradients of the variables involved in the computation.\n",
        "\n",
        "* tape.gradient(loss_value, [W, B]):\n",
        "  * This computes the gradients of the loss_value with respect to the variables [W, B].\n",
        "\n",
        "* optimizer.apply_gradients(zip(gradients, [W, B])):\n",
        "  * This applies the computed gradients to update the weights and biases. The zip function pairs each gradient with its corresponding variable."
      ],
      "metadata": {
        "id": "zBi7oApbo2ar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "no_of_epochs = 1000\n",
        "\n",
        "for n in range(no_of_epochs):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = perceptron(X)  # Compute the logits\n",
        "        loss_value = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits))  # Calculate loss\n",
        "\n",
        "    # Calculate gradients\n",
        "    gradients = tape.gradient(loss_value, [W, B])\n",
        "\n",
        "    # Apply gradients using the optimizer\n",
        "    optimizer.apply_gradients(zip(gradients, [W, B]))"
      ],
      "metadata": {
        "id": "BL_vfagxohgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LtnB2qib4zg"
      },
      "source": [
        "## 10. Print the weights\n",
        "<img alt=\"AI Generated cartoon of happy people eating healthy food.\" src=\"images/happy_people.jpg\" align=\"right\" width=\"300\">\n",
        "\n",
        "Notice that the model has learned that the general happiness of a participant is the best predictor of whether or not they are following the DARN Plan! Of the weights, the 3rd one has the largest value.\n",
        "\n",
        "Given that the input from each feature will be a 0 or a 1, multiplying by a larger weight will increase the contribution of that feature in the summation of all input-by-weight products ($x_i * w_i$) in determining the output of the neuron.\n",
        "\n",
        "The perceptron has learned how to take the three input variables and weigh them to predict the output.\n",
        "\n",
        "**Note:** The image was generated with AI's assistance.\n",
        "\n",
        "```python\n",
        "print(W)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "y9nveon_b4zh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b10c0c6-fd58-48e2-f232-cb87b65a0960"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tf.Variable 'Variable:0' shape=(3, 1) dtype=float32, numpy=\n",
            "array([[0.7009806],\n",
            "       [0.4296667],\n",
            "       [1.6375327]], dtype=float32)>\n"
          ]
        }
      ],
      "source": [
        "print(W)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLEvKraCb4zh"
      },
      "source": [
        "## 11. Print the bias\n",
        "\n",
        "```python\n",
        "print(B)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "9m6M7j5ub4zh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a804461-19d6-4775-f764-7a8eb39b36aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tf.Variable 'Variable:0' shape=(1, 1) dtype=float32, numpy=array([[-0.8423005]], dtype=float32)>\n"
          ]
        }
      ],
      "source": [
        "print(B)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1T0a24qBb4zh"
      },
      "source": [
        "## 12. Test the perceptron\n",
        "\n",
        "The numbers in the output tensor reflect the perceptron's predictions for each input case. These are not probabilities but the **model's estimate of the output value**. We could set a threshold value and conclude the participant is following the DARN Plan when the value exceeds some number.\n",
        "\n",
        "```python\n",
        "print(perceptron(X))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "h0eaP6IRb4zh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6df6e7b3-4751-4a30-c84e-d3e3947bcd01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[ 0.958399  ]\n",
            " [ 0.8410935 ]\n",
            " [ 0.90446156]\n",
            " [ 0.6613629 ]\n",
            " [ 0.2806125 ]\n",
            " [-0.3907067 ]\n",
            " [-0.14038654]\n",
            " [-0.6870257 ]], shape=(8, 1), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "print(perceptron(X))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbvAFoWHb4zh"
      },
      "source": [
        "### Print things more clearly\n",
        "\n",
        "Let's bring the `X`, `y` and predictions together to make it easier to read. Remember that `Yes=1` and `No=0` in the table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "NMpajfLLb4zh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "outputId": "30200f28-b03b-42d5-97f7-17b244c3fe0d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Photos of 3 meals submitted?  Satisfied with food choices?  \\\n",
              "0                           1.0                           1.0   \n",
              "1                           0.0                           1.0   \n",
              "2                           1.0                           0.0   \n",
              "3                           0.0                           0.0   \n",
              "4                           1.0                           1.0   \n",
              "5                           0.0                           1.0   \n",
              "6                           1.0                           0.0   \n",
              "7                           0.0                           0.0   \n",
              "\n",
              "   Generally happy?  Following the DARN Plan?  Predictions  \n",
              "0               1.0                       1.0     0.958399  \n",
              "1               1.0                       1.0     0.841093  \n",
              "2               1.0                       1.0     0.904462  \n",
              "3               1.0                       1.0     0.661363  \n",
              "4               0.0                       1.0     0.280612  \n",
              "5               0.0                       0.0    -0.390707  \n",
              "6               0.0                       0.0    -0.140387  \n",
              "7               0.0                       0.0    -0.687026  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8c6e342c-5f02-460b-b595-85ffc1409dc4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Photos of 3 meals submitted?</th>\n",
              "      <th>Satisfied with food choices?</th>\n",
              "      <th>Generally happy?</th>\n",
              "      <th>Following the DARN Plan?</th>\n",
              "      <th>Predictions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.958399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.841093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.904462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.661363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.280612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.390707</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.140387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.687026</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8c6e342c-5f02-460b-b595-85ffc1409dc4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8c6e342c-5f02-460b-b595-85ffc1409dc4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8c6e342c-5f02-460b-b595-85ffc1409dc4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-136f4bd5-e1cf-44c8-8ea2-5db631d1a73a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-136f4bd5-e1cf-44c8-8ea2-5db631d1a73a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-136f4bd5-e1cf-44c8-8ea2-5db631d1a73a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_fe755a0e-bf18-4f4f-8e02-c6e252c80c03\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_fe755a0e-bf18-4f4f-8e02-c6e252c80c03 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"Photos of 3 meals submitted?\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Satisfied with food choices?\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Generally happy?\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Following the DARN Plan?\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Predictions\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.8410934805870056,\n          -0.3907066881656647\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "X_df = pd.DataFrame(X.numpy(), columns=['Photos of 3 meals submitted?', 'Satisfied with food choices?', 'Generally happy?'])\n",
        "y_df = pd.DataFrame(y.numpy(), columns=['Following the DARN Plan?'])\n",
        "pred_df = pd.DataFrame(perceptron(X).numpy(), columns=['Predictions'])\n",
        "df = pd.concat([X_df, y_df, pred_df], axis=1)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeGDvpV2b4zi"
      },
      "source": [
        "## 13. Let's see how different choices would change the results\n",
        "\n",
        "### Did we run enough epochs?\n",
        "\n",
        "You may want to increase the number of epochs used.\n",
        "\n",
        "### Change the outcomes\n",
        "\n",
        "Usually, we don't change the data we are working with, but in this example, we do this so that you can see the link between the input data and the weights learned. Let's change the participant outcomes and see what happens to the learned weights and predictions.\n",
        "\n",
        "#### Change 1: participants are more likely to follow the DARN Plan when they like the food choices:\n",
        "\n",
        "`y = tf.Variable([1, 1, 0, 0, 1, 0, 0, 0], dtype = tf.float32)`\n",
        "\n",
        "#### Change 2: participants are more likely to follow the DARN Plan when they regularly submit three photos a day:\n",
        "\n",
        "`y = tf.Variable([1, 0, 1, 0, 1, 0, 1, 0], dtype = tf.float32)`\n",
        "\n",
        "Feel free to play with other parts of the model; everything but the X inputs is replicated below to put it all in one place for easy reference. Comments point out hyperparameters that you might want to change."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "sl2NKlleb4zi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "fa97e41c-124d-46bf-d5f2-54c7a53b01d7"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'SGD' object has no attribute 'minimize'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-b9f2c266013c>\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_of_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperceptron\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m## From steps 10 on, printing the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'SGD' object has no attribute 'minimize'"
          ]
        }
      ],
      "source": [
        "## From step 3\n",
        "# Outputs:       1, 2, 3, 4, 5, 6, 7, 8 - one for each case in the table\n",
        "y = tf.Variable([1, 1, 0, 0, 1, 0, 0, 0], dtype = tf.float32)  # Change 1 has been made, you'll need to make change 2\n",
        "y = tf.reshape(y, [8,1])  # convert to 4x1\n",
        "\n",
        "## From step 4\n",
        "num_features = X.shape[1]\n",
        "output_size = 1\n",
        "\n",
        "## From step 5\n",
        "W = tf.Variable(tf.zeros([num_features, output_size]), dtype = tf.float32)\n",
        "\n",
        "## From step 6\n",
        "B = tf.Variable(tf.zeros([output_size, 1]), dtype = tf.float32)\n",
        "\n",
        "## From step 7\n",
        "def perceptron(X):\n",
        "    z = tf.add(tf.matmul(X, W), B)\n",
        "    output = tf.tanh(z)                  # Activation function is a good hyperparameter to change\n",
        "    return output\n",
        "\n",
        "## From step 8\n",
        "learning_rate = 0.01  # Learning rate is a good hyperparameter to change\n",
        "optimizer = tf.optimizers.SGD(learning_rate)\n",
        "\n",
        "## From step 9\n",
        "no_of_epochs = 1000  # Number of epochs is a good hyperparameter to change\n",
        "\n",
        "for n in range(no_of_epochs):\n",
        "    loss = lambda:abs(tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels = y, logits = perceptron(X))))\n",
        "    optimizer.minimize(loss, [W, B])\n",
        "\n",
        "## From steps 10 on, printing the output\n",
        "print(f'Weights: {W}')\n",
        "print(f'Bias: {B}')\n",
        "\n",
        "X_df = pd.DataFrame(X.numpy(), columns=['Photos of 3 meals submitted?', 'Satisfied with food choices?', 'Generally happy?'])\n",
        "y_df = pd.DataFrame(y.numpy(), columns=['Following the DARN Plan?'])\n",
        "pred_df = pd.DataFrame(perceptron(X).numpy(), columns=['Predictions'])\n",
        "df = pd.concat([X_df, y_df, pred_df], axis=1)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fC31sCAlb4zi"
      },
      "source": [
        "## Before continuing\n",
        "###  <img src='images/alert_icon.svg' alt=\"Alert icon\" width=40 align=center> Alert!\n",
        "> Before continuing to another notebook within the same Jupyter session,\n",
        "> use the **\"Running Terminals and Kernels\" tab** (below the File Browser tab) to **shut down this kernel**.\n",
        "> This will free up this notebook's GPU memory, making it available for\n",
        "> your next notebook.\n",
        ">\n",
        "> Every time you run multiple notebooks within a Jupyter session with\n",
        "> a GPU, this should be done.\n",
        "\n",
        "----\n",
        "## Push changes to GitHub <img src=\"images/push_to_github.png\" alt=\"Push to GitHub icon\" align=\"right\" width=150>\n",
        "\n",
        " Remember to **add**, **commit**, and **push** the changes you have made to this notebook to GitHub to keep your repository in sync.\n",
        "\n",
        "In Jupyter, those are done in the git tab on the left. In Google Colab, use File > Save a copy in GitHub.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asheLiTnb4zi"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Tensorflow-2.15",
      "language": "python",
      "name": "tensorflow-2.15"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}