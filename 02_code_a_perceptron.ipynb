{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Practicum AI Logo image](https://github.com/PracticumAI/practicumai.github.io/blob/main/images/logo/PracticumAI_logo_250x50.png?raw=true) <img src='https://github.com/PracticumAI/deep_learning/blob/main/images/deep_learning_foundations.png?raw=true' align='right' width=50 padding=50>\n",
    "***\n",
    "# *Practicum AI:* Deep Learning - Perceptron\n",
    "\n",
    "\n",
    "> This exercise adapted from the [W3 Schools Perceptrons](https://www.w3schools.com/ai/ai_perceptrons.asp) article and from Baig et al. (2020) The Deep Learning Workshop from [Packt Publishers](https://www.packtpub.com/product/the-deep-learning-workshop/9781839219856) (Exercise 2.01, page 55).\n",
    "\n",
    "<img alt=\"A cartoon of Dr. Amelia, a nutrition researcher, sitting at a computer thinking about food items which appear in a thought bubble.\" src=\"images/DrAmelia.jpg\" align=\"right\" width=250>Amelia is back! This time, she needs your help to analyze some of her survey data. We'll use a simple [perceptron](https://developers.google.com/machine-learning/glossary#perceptron) to predict if patients follow her special Dr. Amelia Recommended Nutrition Plan (the DARN Plan!). \n",
    "\n",
    "**Note:** Dr. Amelia's cartoon was generated with AI's assistance.\n",
    " \n",
    "As a note, this exercise lies somewhere between coding everything from scratch and relying on the pre-coded APIs (Application Programming Interfaces) that underlie the power of TensorFlow, Keras, and Pytorch. **You will not need to create weight tensors beyond this exercise**. Still, hopefully, by doing it this time, you will have a better understanding (*and appreciation*) of the details often lost in an API call to `model.fit()`, for example.\n",
    "\n",
    "The table below shows some data Amelia's gathered from patient surveys about their nutrition. She's looking at how different factors predict if patients follow her DARN Plan ($y$, the output or [labels](https://developers.google.com/machine-learning/glossary#label) in our example) based on three input variables: if patients submit photos of three meals a day ($x_1$), if patients report being satisfied with their food choices ($x_2$), and if patients report being generally happy  ($x_3$). We will combine $x_1$, $x_2$, and $x_3$ into our input tensor $X$. Here, we are simplifying the question of the likelihood of following the DARN Plan to a Yes/No. \n",
    "\n",
    "Case # | Photos of 3 meals submitted? ($x_1$) | Satisfied with food choices? ($x_2$) | Generally happy? ($x_3$) | Following the DARN Plan? ($y$)\n",
    "--|--------------------------|---------------------|-----------------------|----------------\n",
    "1 | 1 (Yes) | 1 (Yes) | 1 (Yes) | Yes (1)\n",
    "2 | 0 (No) | 1 (Yes) | 1 (Yes) | Yes (1)\n",
    "3 | 1 (Yes) | 0 (No) | 1 (Yes) | Yes (1)\n",
    "4 | 0 (No) | 0 (No) | 1 (Yes) | Yes (1)\n",
    "5 | 1 (Yes) | 1 (Yes) | 0 (No) | Yes (1)\n",
    "6 | 0 (No) | 1 (Yes) | 0 (No) | No (0)\n",
    "7 | 1 (Yes) | 0 (No) | 0 (No) | No (0)\n",
    "8 | 0 (No) | 0 (No) | 0 (No) | No (0)\n",
    "\n",
    "\n",
    "## 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow.keras import activations\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create an input data matrix\n",
    "\n",
    "Create a 3 x 8 matrix for our input data. Remember that we have three inputs (we'll call them $x_1$, $x_2$, and $x_3$ for now), these are the columns in our input data.\n",
    "\n",
    "The matrix below has the three input columns of our data table, using just the 0/1 values corresponding to the no/yes entries in the table. The comments help line up rows of the table with entries in our `X` variable. (Remember, we are using the capital letter `X` as our variable name here to remind us that this is a matrix with our input data).\n",
    "\n",
    "We'll probably stop reminding you after this, but...remember not all red output is bad!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.Variable([[1.,1.,1.], # Case 1\n",
    "                 [0.,1.,1.], # Case 2\n",
    "                 [1.,0.,1.], # Case 3\n",
    "                 [0.,0.,1.], # Case 4\n",
    "                 [1.,1.,0.], # Case 5\n",
    "                 [0.,1.,0.], # Case 6\n",
    "                 [1.,0.,0.], # Case 7\n",
    "                 [0.,0.,0.]], # Case 8\n",
    "                 dtype = tf.float32)  # 3x8, input data table\n",
    "print(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create a label tensor\n",
    "\n",
    "Create a tensor of labels to hold our 'ground truth'. This is the decision for each set of input whether or not the patient is following the DARN Plan. \n",
    "\n",
    "```python\n",
    "# Outputs:       1, 2, 3, 4, 5, 6, 7, 8--one for each case in the table         \n",
    "y = tf.Variable([1, 1, 1, 1, 1, 0, 0, 0], dtype = tf.float32) \n",
    "\n",
    "y = tf.reshape(y, [8,1]) # Reshape to be 8 rows of 1 column  \n",
    "print(y)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define some constants to set the shape of the weight matrix\n",
    "\n",
    "Define two constants to be used in the next step when we define the connections weight matrix.\n",
    "\n",
    "We can use the number of columns in the X table to determine the number of features or how many $x_i$'s we have and, therefore, how many weights we need to store (one for each feature). We only need one output value since we are looking for a binary decision about good turnout (Yes/No)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "num_features = X.shape[1]\n",
    "output_size = 1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## 5. Define connections weight matrix\n",
    "\n",
    "![Diagram of the perceptron with 3 input variables (x1, x2, x3), 3 weights (w1, W2, w3) and the bias term. The perceptron body multiplies the inputs by the weights and sums them and the bias, resulting in the output--whether or not the patient is following the DARN Plan. The three weights are highlighted here.](images/02_perceptron_section5.png)\n",
    "\n",
    "In our feature matrix, we will need one weight for each feature, $x_i$ (three photos submitted, satisfied with food choices, etc.), labeled $X$. These weights are our $w_i$'s. We don't know what value they should take so that we can initialize them to 0. Another common option is to use a random number to initialize the weights--this is one reason different runs of model training may give different answers.\n",
    "\n",
    "```python\n",
    "W = tf.Variable(tf.zeros([num_features, output_size]), dtype = tf.float32)\n",
    "print(W)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## 6. Define bias variable\n",
    "\n",
    "![Diagram of the perceptron with 3 input variables (x1, x2, x3), 3 weights (w1, W2, w3) and the bias term. This is similar to the above image, but is highlighting the bias term](images/02_perceptron_section6.png)\n",
    "\n",
    "Since we only have one neuron, we only need one bias value. Again, we'll initialize it to 0--a random number would be another option here. We can write each bias term as $b_i$ and the matrix of all biases as $B$.\n",
    "\n",
    "```python\n",
    "B = tf.Variable(tf.zeros([output_size, 1]), dtype = tf.float32)\n",
    "print(B)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## 7. Define a perceptron function\n",
    "\n",
    "![Diagram of the perceptron with 3 input variables (x1, x2, x3), 3 weights (w1, W2, w3) and the bias term. This is similar to the above image, but is highlighting the perceptron body.](images/02_perceptron_section7.png)\n",
    "\n",
    "\n",
    "In the following code block, we define a perceptron function with one input argument, $X$, containing our three input data features. \n",
    "\n",
    "The function's first line implements a net input function.  It multiplies the input data matrix ($X$) by the weights ($W$) using the matrix multiplication function (matmul).  It then adds the bias ($B$) value to that product.\n",
    "\n",
    "### <img src='images/note_icon.svg' width=40, align='center' alt='Note icon'>Note\n",
    "> This is the essential function of a neuron: gather the inputs, multiply each input by the weight for that input, add the products up and add in the bias.\n",
    "\n",
    "The function's second line implements an activation function. The activation function determines how the neuron's output (calculated above) is changed before passing it on. Here, we use the `tanh` activation function.  However, there are other TensorFlow options.  For example, you could use the `tf.sigmoid` function.  Or, select a function from the Keras activation (`activations`) library.  Search the [Keras documentation](https://keras.io/api/layers/activations/) for a complete list of available functions.\n",
    "\n",
    "Try out these other options, retrain the network, and see what happens.\n",
    "\n",
    "```python\n",
    "output = tf.sigmoid(z)\n",
    "output = activations.relu(z)\n",
    "output = activations.linear(z)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron(X):\n",
    "    z = tf.add(tf.matmul(X, W), B)  # Net input function\n",
    "    output = tf.tanh(z)             # Activation function\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the perceptron function to see its initial predictions before any training.  All of its predictions ought to be 0 (remember we set all the weights and the bias to 0--so whatever the inputs are, they are all multiplied by 0 and have 0 added to the sum). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(perceptron(X))        # Execute the perceptron to see its initial predictions before training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training the Perceptron\n",
    "\n",
    "Now that we have the elements of a simple, single-node perceptron in place, let's train the network using backpropagation. The optimizer algorithm implements the backpropagation, so we don't need to code that ourselves.\n",
    "\n",
    "The [learning rate](https://developers.google.com/machine-learning/glossary#learning-rate) determines the size of the steps taken towards the global minimum, while the optimizer manages the weight update process during backpropagation.  Here, the Stochastic Gradient Descent (SGD) optimizer has been selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "optimizer = tf.optimizers.SGD(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train the perceptron for 1000 epochs\n",
    "\n",
    "An [epoch](https://developers.google.com/machine-learning/glossary#epoch) is a complete training pass over the entire dataset.  Our loss or error function is defined as a lambda function (a single-line, inline function) in the first line of code in the loop block.  We use the `sigmoid_cross_entropy_with_logits` function, an appropriate choice for this application, to calculate how far our predicted results are from the known results. We will not get into the technical details here as that is outside the scope of this learning experience. Our SGD optimizer seeks to minimize the model's total error in the second line.\n",
    "\n",
    "### <img src='images/note_icon.svg' width=40, align='center' alt='Note icon'>Note\n",
    "> The code below uses a `for` loop. This common programming construct allows you to loop, or iterate, through\n",
    "> a list of items (the numbers 0 to 999 in our case). *Implicitly*, training will use `for` loops--for each epoch do \n",
    "> this thing. *Explicitly*, however, after this notebook, we will use the API that automatically does this for us.\n",
    "> Thus we dropped coverage of `for` loops and other \"flow control\" methods from the *Python for AI* course. It's\n",
    "> helpful to know about them, but they are rarely used explicitly in AI research.\n",
    "> [Click here for more details](https://wiki.python.org/moin/ForLoop).\n",
    ">\n",
    "> The block also uses a special Python function called a `lambda` function. These are functions that can be \n",
    "> written as a single line of code. [Click here for more\n",
    "> details](https://docs.python.org/3/tutorial/controlflow.html#lambda-expressions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "no_of_epochs = 1000\n",
    "\n",
    "for n in range(no_of_epochs):\n",
    "    loss = lambda:abs(tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels = y, logits = perceptron(X))))\n",
    "    optimizer.minimize(loss, [W, B])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Print the weights\n",
    "<img alt=\"AI Generated cartoon of happy people eating healthy food.\" src=\"images/happy_people.jpg\" align=\"right\" width=\"300\">\n",
    "\n",
    "Notice that the model has learned that the general happiness of a patient is the best predictor of whether or not they are following the DARN Plan! Of the weights, the 3rd one has the largest value.\n",
    "\n",
    "Given that the input from each feature will be a 0 or a 1, multiplying by a larger weight will increase the contribution of that feature in the summation of all input-by-weight products ($x_i * w_i$) in determining the output of the neuron.\n",
    "\n",
    "The perceptron has learned how to take the three input variables and weigh them to predict the output. \n",
    "\n",
    "**Note:** The image was generated with AI's assistance.\n",
    "\n",
    "```python\n",
    "print(W)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Print the bias\n",
    "\n",
    "```python\n",
    "print(B)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Test the perceptron\n",
    "\n",
    "The numbers in the output tensor reflect the perceptron's predictions for each input case. These are not probabilities but the **model's estimate of the output value**. We could set a threshold value and conclude the patient is following the DARN Plan when the value exceeds some number.\n",
    "\n",
    "```python\n",
    "print(perceptron(X))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print things more clearly\n",
    "\n",
    "Let's bring the `X`, `y` and predictions together to make it easier to read. Remember that `Yes=1` and `No=0` in the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = pd.DataFrame(X.numpy(), columns=['Photos of 3 meals submitted?', 'Satisfied with food choices?', 'Generally happy?'])\n",
    "y_df = pd.DataFrame(y.numpy(), columns=['Following the DARN Plan?'])\n",
    "pred_df = pd.DataFrame(perceptron(X).numpy(), columns=['Predictions'])\n",
    "df = pd.concat([X_df, y_df, pred_df], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Let's see how different choices would change the results\n",
    "\n",
    "Let's change the patient outcomes and see what happens to the learned weights and predictions. \n",
    "\n",
    "### Change 1: Patients are more likely to follow the DARN Plan when they like the food choices:\n",
    "\n",
    "`y = tf.Variable([1, 1, 0, 0, 1, 0, 0, 0], dtype = tf.float32)`\n",
    "\n",
    "### Change 2: Patients are more likely to follow the DARN Plan when they regularly submit three photos a day:\n",
    "\n",
    "`y = tf.Variable([1, 0, 1, 0, 1, 0, 1, 0], dtype = tf.float32)`\n",
    "\n",
    "Feel free to play with other parts of the model; everything but the X inputs is replicated below to put it all in one place for easy reference. Comments point out hyperparameters that you might want to change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## From step 3\n",
    "# Outputs:       1, 2, 3, 4, 5, 6, 7, 8--one for each case in the table         \n",
    "y = tf.Variable([1, 1, 0, 0, 1, 0, 0, 0], dtype = tf.float32) # Change 1 has been made, you'll need to make change 2\n",
    "y = tf.reshape(y, [8,1])  # convert to 4x1\n",
    "\n",
    "## From step 4\n",
    "num_features = X.shape[1]\n",
    "output_size = 1\n",
    "\n",
    "## From step 5\n",
    "W = tf.Variable(tf.zeros([num_features, output_size]), dtype = tf.float32)\n",
    "\n",
    "## From step 6\n",
    "B = tf.Variable(tf.zeros([output_size, 1]), dtype = tf.float32)\n",
    "\n",
    "## From step 7\n",
    "def perceptron(X):\n",
    "    z = tf.add(tf.matmul(X, W), B)      \n",
    "    output = tf.tanh(z)                  # Activation function is a good hyperparameter to change \n",
    "    return output\n",
    "\n",
    "## From step 8\n",
    "learning_rate = 0.01  # Learning rate is a good hyperparameter to change\n",
    "optimizer = tf.optimizers.SGD(learning_rate)\n",
    "\n",
    "## From step 9\n",
    "no_of_epochs = 1000 # Number of epochs is a good hyperparameter to change\n",
    "\n",
    "for n in range(no_of_epochs):\n",
    "    loss = lambda:abs(tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels = y, logits = perceptron(X))))\n",
    "    optimizer.minimize(loss, [W, B])\n",
    "    \n",
    "## From steps 10 on, printing the output\n",
    "print(f'Weights: {W}')\n",
    "print(f'Bias: {B}')\n",
    "\n",
    "X_df = pd.DataFrame(X.numpy(), columns=['Photos of 3 meals submitted?', 'Satisfied with food choices?', 'Generally happy?'])\n",
    "y_df = pd.DataFrame(y.numpy(), columns=['Following the DARN Plan?'])\n",
    "pred_df = pd.DataFrame(perceptron(X).numpy(), columns=['Predictions'])\n",
    "df = pd.concat([X_df, y_df, pred_df], axis=1)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow-2.6.0",
   "language": "python",
   "name": "tensorflow-2.6.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
